{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Question Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "loadpath = \"processed_data_bert_expand\"\n",
    "bert_data_path = \"bert_expand.pkl\"\n",
    "analysis_output_path = \"related_question_analysis\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dataset with bert sentence representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bert_data_path, \"rb\") as f:\n",
    "    bert_data = pickle.load(f)\n",
    "clean_data = bert_data[\"clean_data\"]\n",
    "reduced_data = bert_data[\"reduced_data\"]\n",
    "token_data = bert_data[\"token_data\"]\n",
    "bert_output = bert_data[\"bert_data\"]\n",
    "\n",
    "print(\"Type: \", type(bert_output), bert_output.shape)\n",
    "bert_tensor = torch.from_numpy(bert_output).to(device)\n",
    "print(bert_tensor.size())\n",
    "bert_norm = bert_tensor / torch.norm(bert_tensor, dim=1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "這邊模擬 testing 的情境，當有一個新的 query sentence，要先把句子預處理完後再丟進 `predict()` function。這邊我偷懶就直接拿之前已經預處理好的句子丟進去。\n",
    "\n",
    "要注意要執行 `predict()` function 前還是要在 shell 用 `bert-serving-start` 把 model run 起來。\n",
    "\n",
    "`predict()` function 會先取得 query sentence 的 sentence representation，接著再與先前 dataset 取得的 Bert sentence representation 去算 cosine similarity，數值越高就與現在這個 query sentence 越相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = os.path.join(\"..\",\"data\",\"./newdata_clean.xlsx\")\n",
    "\n",
    "df = pd.read_excel(path).drop_duplicates(subset=\"question\", keep='last')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['catName'].unique())\n",
    "num_classes = len(le.classes_)\n",
    "class_list = list(le.classes_)\n",
    "\n",
    "for i, c in enumerate(class_list):\n",
    "    print(\"{}: {}\".format(i, c))\n",
    "\n",
    "print(\"number of classes:\",num_classes)\n",
    "df.loc[:,'catName'] = le.transform(df.loc[:,'catName'])\n",
    "data = df[['question', 'catName']]\n",
    "\n",
    "data.set_index('question',inplace=True)\n",
    "print(data.head())\n",
    "data = data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_index(index, num_related=3, verbal=False):\n",
    "    if verbal:\n",
    "        print(\"Query: {}\".format(clean_data[index]))\n",
    "    \n",
    "    similarity = torch.matmul(bert_norm, bert_norm[index].view(-1,1))\n",
    "    rank = torch.argsort(similarity, dim=0, descending=True)\n",
    "    ret = []\n",
    "    for i in range(1, num_related + 1):\n",
    "        ret.append(reduced_data[rank[i]])\n",
    "        if verbal:\n",
    "            print(\"\\n\" + \"=\" * 10 + \"Similarity: {}\".format(similarity[rank[i]][0]) + \"=\" * 10)\n",
    "            print(re.sub(r'<[^<]*?/?>', '', reduced_data[rank[i]])) # remove output sentence html \n",
    "            #print()\n",
    "            #print(clean_data[rank[i]])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_list = []\n",
    "predict_list = []\n",
    "\n",
    "for index in trange(len(clean_data)):\n",
    "    ret = predict_index(index, verbal=False)\n",
    "    gt = data.loc[reduced_data[index]]['catName']\n",
    "    #print(gt)\n",
    "    gt_list.append(gt)\n",
    "    predict_class_index = list(map(lambda s: data.loc[s]['catName'], ret))\n",
    "    #print(predict_class_index)\n",
    "    predict_list.append(predict_class_index)\n",
    "\n",
    "print(\"clean_data:\", len(clean_data))\n",
    "print(\"gt_list:\", len(gt_list))\n",
    "print(\"predict_list:\", len(predict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_data = {\n",
    "    \"\"\n",
    "    \"gt\": gt_list,\n",
    "    \"predict\": predict_list,\n",
    "}\n",
    "with open(analysis_output_path, \"wb\") as f:\n",
    "    pickle.dump(analysis_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(analysis_output_path, \"rb\") as f:\n",
    "    analysis_data = pickle.load(f)\n",
    "gt_list = analysis_data[\"gt\"]\n",
    "predict_list = analysis_data[\"predict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(predicts, gt, labels):\n",
    "    matrix = confusion_matrix(gt, predicts, labels=labels)\n",
    "    df = pd.DataFrame(matrix, columns=labels, index=labels)\n",
    "    #print(df.head())\n",
    "    \n",
    "    plt.figure(figsize=(35, 15))\n",
    "    plt.title('Confusion Matrix', y=1.03, fontsize = 25)\n",
    "    #cmap = sn.cubehelix_palette(start = 1.5, rot = 3, gamma=0.8, as_cmap = True)\n",
    "    heatmap = sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # , cmap=cmap\n",
    "    plt.ylabel('Ground Truth', fontsize = 20)\n",
    "    plt.xlabel('Prediction', fontsize = 20)\n",
    "    heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=45, horizontalalignment=\"right\")\n",
    "    plt.savefig('ConfusionMatrix_newdata.jpg', bbox_inches = \"tight\")\n",
    "    \n",
    "    normalized_matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis] # normalize\n",
    "    normalized_matrix = np.nan_to_num(normalized_matrix).round(2)\n",
    "    df = pd.DataFrame(normalized_matrix, columns=labels, index=labels)\n",
    "    \n",
    "    plt.figure(figsize=(35, 15))\n",
    "    plt.title('Normalized Confusion Matrix', y=1.03, fontsize = 25)\n",
    "    #cmap = sn.cubehelix_palette(start = 1.5, rot = 3, gamma=0.8, as_cmap = True)\n",
    "    heatmap = sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # , cmap=cmap\n",
    "    plt.ylabel('Ground Truth', fontsize = 20)\n",
    "    plt.xlabel('Prediction', fontsize = 20)\n",
    "    heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=45, horizontalalignment=\"right\")\n",
    "    plt.savefig('Normalized_ConfusionMatrix_newdata.jpg', bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "    n = 0\n",
    "    n_correct = 0\n",
    "    '''\n",
    "    for i in range(len(gt)):\n",
    "        n += 1\n",
    "        if gt[i][maxindex[i]] == 1:\n",
    "            n_correct += 1\n",
    "    print(\"Accuracy: {}\".format(n_correct / n))\n",
    "    print(len(matrix), len(matrix[0]))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict_list_first = np.array(predict_list)[:,0]\n",
    "\n",
    "predict_str = list(le.inverse_transform(predict_list_first))\n",
    "gt_str = list(le.inverse_transform(gt_list))\n",
    "    \n",
    "analysis(predict_str, gt_str, labels=class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "從最後輸出的結果來看其實還不錯，可是再經過一些調查後發現 bert 並不適合這樣直接當作 sentence encoder，目前想到的解決方法如下。\n",
    "\n",
    "1. 先 find tune 在一些 task 上，像是最一開始做得 supervised classification 後再拿 `[CLS]` 的 output 作為 sentence represention。\n",
    "2. 使用 [Universal Sentence Encoder](https://arxiv.org/pdf/1803.11175.pdf)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
