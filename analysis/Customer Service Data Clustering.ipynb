{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Service Data Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"..\",\"data\",\"./newdata_clean.xlsx\")\n",
    "n_cpu = 15\n",
    "batch_size = 10000\n",
    "max_k = 40\n",
    "max_features = 256  # only consider the top max_features ordered by term frequency across the corpus.\n",
    "loadpath = \"processed_data_not_rmsw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(path)\n",
    "\n",
    "df = df.dropna() # drop nan entry\n",
    "# df[pd.isnull(df).any(axis=1)]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['catName'].unique())\n",
    "num_classes = len(le.classes_)\n",
    "class_list = list(le.classes_)\n",
    "\n",
    "#print(class_list)\n",
    "\n",
    "print(\"number of classes:\",num_classes)\n",
    "df.loc[:,'catName'] = le.transform(df.loc[:,'catName'])\n",
    "data = df.question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords length: 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/student/05/b05505004/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student/05/b05505004/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "import langid\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stw = stopwords.words('english') + ['nbsp', 'powerdirector', 'cyberlink', 'powerdvd', 'power', 'director', 'ba']\n",
    "#print(stw)\n",
    "print(\"Stopwords length: {}\".format(len(stw)))\n",
    "\n",
    "def _filter(ori_x):\n",
    "    x = re.sub('<[^<]*?/?>', ' ', ori_x)        # remove all html tag\n",
    "    x = re.sub('https?:\\/\\/[^ ]*', ' ', x)  # remove all url\n",
    "    x = re.sub('\\S*@\\S*\\s?', ' ', x)        # remove all email address\n",
    "    x = re.sub('\\S*\\.\\S*\\s?', ' ', x, flags=re.IGNORECASE)        # remove all filename\n",
    "    x = re.sub('[^a-z A-Z]', ' ', x)        # remove all non-english alphabat\n",
    "    return x\n",
    "'''\n",
    "def _correct_word(text1):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    text2 = pattern.sub(r\"\\1\\1\", text1) # reduce lengthening\n",
    "    #if text1 != text2:\n",
    "    #    print(text1, text2)\n",
    "    text3 = spell(text2).lower() # spell correction\n",
    "    #if text2 != text3:\n",
    "    #    print(text2, text3)\n",
    "    return text3\n",
    "'''\n",
    "def _get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def _lemmatization(tokens):\n",
    "    tagged_sent = pos_tag(tokens)   \n",
    "    ret = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = _get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        ret.append(wnl.lemmatize(tag[0], pos=wordnet_pos))\n",
    "    return ret\n",
    "\n",
    "def _remove_stopword(tokens):\n",
    "    ret = []\n",
    "    for word in tokens:\n",
    "        if word not in stw and len(word) > 2:\n",
    "            ret.append(word)\n",
    "    return ret\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence = _filter(sentence.lower())\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    #tokens = [self._correct_word(word) for word in tokens] # spell correction\n",
    "    tokens = _lemmatization(tokens) # lemmatization\n",
    "    #tokens = _remove_stopword(tokens) # remove stopwords\n",
    "    s = \" \".join(tokens)\n",
    "\n",
    "    return s, tokens\n",
    "\n",
    "def process_batch(batch):   \n",
    "    clean_batch = []\n",
    "    reduced_batch = []\n",
    "    token_batch = []\n",
    "    for s in tqdm(batch):\n",
    "        ret = langid.classify(s)\n",
    "        if ret[0] != \"en\" and ret[1] < -100: # remove language other than english\n",
    "            #print(ret)\n",
    "            #print(s)\n",
    "            continue\n",
    "        else:\n",
    "            processed, tokens = preprocess(s)\n",
    "            if len(tokens) <= 3: # remove too short sentence\n",
    "                print(s,processed)\n",
    "                continue\n",
    "            #print(processed)\n",
    "            #print(tokens)\n",
    "            reduced_batch.append(s)\n",
    "            clean_batch.append(processed)\n",
    "            token_batch.append(tokens)\n",
    "    return clean_batch, reduced_batch, token_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/7098 [00:05<1:44:26,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "help<br><br>Attach File : <a href='http://csupload.cyberlink.com/upload-file/support/cs/2018-09-23/CS001936718/CL_PHOTODIRECTOR_DATA.zip' target=_blank>CL_PHOTODIRECTOR_DATA.zip</a> help attach file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/7106 [00:06<13:08:42,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dudihovkcv<br><br>Attach File : <a href='http://csupload.cyberlink.com/upload-file/support/cs/2018-11-12/CS001953678/feedback_data.zip' target=_blank>feedback_data.zip</a> dudihovkcv attach file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 91/7098 [00:08<05:56, 19.65it/s]t]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi<br><br>Attach File : <a href='http://csupload.cyberlink.com/upload-file/support/cs/2018-02-05/CS001854032/CL_PHOTODIRECTOR_DATA.zip' target=_blank>CL_PHOTODIRECTOR_DATA.zip</a> hi attach file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/7098 [00:08<1:51:55,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿<html><div dir=\"auto\"> <br> <br> <div data-smartmail=\"gmail_signature\"> A</div> </div>   </html><br/>﻿ a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 103/7098 [00:08<06:21, 18.36it/s]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I Like you i like you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/7098 [00:09<5:59:20,  3.04s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great<br><br>Attach File : <a href='http://csupload.cyberlink.com/upload-file/support/cs/2018-09-18/CS001935208/CL_PHOTODIRECTOR_DATA.zip' target=_blank>CL_PHOTODIRECTOR_DATA.zip</a> great attach file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 140/7098 [00:10<06:28, 17.92it/s]]Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-13:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-169045700e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCLOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In unknown state\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clean_data = []\n",
    "reduced_data = []\n",
    "token_data = []\n",
    "\n",
    "n_workers = n_cpu\n",
    "from multiprocessing import Pool\n",
    "ret = [None] * n_workers\n",
    "n_data = len(data)\n",
    "with Pool(processes=n_workers) as pool:\n",
    "    for i in range(n_workers):\n",
    "        batch_start = (n_data // n_workers) * i \n",
    "        if i == n_workers - 1:\n",
    "            batch_end = n_data\n",
    "        else:\n",
    "            batch_end = (n_data // n_workers) * (i + 1)\n",
    "        batch = data[batch_start:batch_end]\n",
    "        ret[i] = pool.apply_async(process_batch, [batch])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "for result in ret:\n",
    "    clean_batch, reduced_batch, token_batch = result.get()\n",
    "    clean_data += clean_batch\n",
    "    reduced_data += reduced_batch\n",
    "    token_data += token_batch\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(data, max_k, batch_size):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    bar = tqdm(iters)\n",
    "    for k in bar:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, batch_size=batch_size, random_state=20).fit(data).inertia_)\n",
    "        bar.set_description('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_plot(data, cluster_algo, n_clusters, eps, calculate_portion, draw_portion):\n",
    "    n_data = data.shape[0]\n",
    "    print(\"Input Data shape:\", data.shape)\n",
    "    \n",
    "    # random sample (n_sample) points\n",
    "    np.random.seed(5)\n",
    "    n_sample = n_data // calculate_portion\n",
    "    print(\"Only calculate {} data points\".format(n_sample))\n",
    "    sample_items = np.random.choice(range(n_data), size=n_sample, replace=False)\n",
    "    \n",
    "    \n",
    "    if cluster_algo == \"kmeans\":\n",
    "        labels = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, random_state=20).fit_predict(data)\n",
    "        npdata = data[sample_items,:]\n",
    "        label_subset = labels[sample_items]\n",
    "    elif cluster_algo == \"dbscan\":\n",
    "        npdata = data[sample_items]\n",
    "        labels = DBSCAN(eps=eps, n_jobs=n_cpu).fit_predict(npdata)\n",
    "        label_subset = labels\n",
    "        print(\"Number of noise data: {}\".format(len(label_subset[label_subset == -1])))\n",
    "    \n",
    "    # number of clusters\n",
    "    max_label = max(labels) \n",
    "    print(\"Number of clusters: {}\".format(max_label + 1))\n",
    "    pca = PCA(n_components=2, whiten=True).fit_transform(npdata)\n",
    "    print(\"PCA done\")\n",
    "    tsne_pca = TSNE(n_jobs=n_cpu, n_iter=5000).fit_transform(PCA(n_components=(max_features // 2)).fit_transform(npdata))\n",
    "    print(\"tsne_pca done\")\n",
    "    tsne = TSNE(n_jobs=n_cpu, n_iter=5000).fit_transform(npdata)\n",
    "    print(\"tsne30 done\")\n",
    "    tsne_per50 = TSNE(n_jobs=n_cpu, perplexity=50, n_iter=5000).fit_transform(npdata)\n",
    "    print(\"tsne50 done\")\n",
    "    \n",
    "    # draw only (n_draw) points\n",
    "    np.random.seed(5)\n",
    "    n_draw = n_sample // draw_portion\n",
    "    print(\"Only draw {} data points\".format(n_draw))\n",
    "    idx = np.random.choice(range(pca.shape[0]), size=n_draw, replace=False)\n",
    "    \n",
    "    # draw scatter\n",
    "    if cluster_algo == \"kmeans\":\n",
    "        start_idx = 0\n",
    "    else:\n",
    "        start_idx = -1\n",
    "    f, ax = plt.subplots(2, 2, figsize=(13, 10))\n",
    "    for i in range(start_idx,max_label + 1):\n",
    "        sub_idx = idx[label_subset[idx] == i]\n",
    "        #print(\"{} points in group {}\".format(len(sub_idx),i))\n",
    "        label_subset_color = np.array([cm.hsv(i/ ( max_label + 1)) for i in label_subset[sub_idx]])\n",
    "        ax[0,0].scatter(pca[sub_idx, 0], pca[sub_idx, 1])   # , c=label_subset_color\n",
    "        ax[0,0].set_title('PCA Cluster Plot')\n",
    "        \n",
    "        ax[1,0].set_title('t-SNE & PCA Cluster Plot')\n",
    "        ax[1,0].scatter(tsne_pca[sub_idx, 0], tsne_pca[sub_idx, 1], label=\"Group {} | {}\".format(i,len(sub_idx))) # , c=label_subset_color\n",
    "        \n",
    "        ax[0,1].set_title('t-SNE Cluster Plot')\n",
    "        ax[0,1].scatter(tsne[sub_idx, 0], tsne[sub_idx, 1]) # , c=label_subset_color\n",
    "        \n",
    "        ax[1,1].set_title('t-SNE Cluster Plot (Perplexity 50)')\n",
    "        ax[1,1].scatter(tsne_per50[sub_idx, 0], tsne_per50[sub_idx, 1]) # , c=label_subset_color\n",
    "    f.legend() # plot only one legend\n",
    "    \n",
    "    if cluster_algo == \"kmeans\":\n",
    "        return data, labels\n",
    "    else:\n",
    "        return npdata, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    \"clean_data\": clean_data,\n",
    "    \"reduced_data\": reduced_data,\n",
    "    \"token_data\": token_data\n",
    "}\n",
    "with open(loadpath, \"wb\") as f:\n",
    "    pickle.dump(output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(loadpath, \"rb\") as f:\n",
    "    output = pickle.load(f)\n",
    "clean_data = output[\"clean_data\"]\n",
    "reduced_data = output[\"reduced_data\"]\n",
    "token_data = output[\"token_data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max_feature\",max_features)\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df = 0.001,\n",
    "    max_df = 0.95,\n",
    "    max_features = max_features,\n",
    "    stop_words = 'english'\n",
    ")\n",
    "tfidf.fit(clean_data)\n",
    "text = tfidf.transform(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_clusters(text, max_k, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text, sample_text_labels = cluster_and_plot(text, \"kmeans\", 10, None, 500, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_and_plot(text, \"dbscan\", None, 0.999, 50, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get keywords of each TF-IDF cluster\n",
    "作法是將每個 cluster 的前幾高 tfidf 字取出來，作為這個 cluster 的代表字。但是效果不好，且很多 cluster 都有相同常見的字(ex: download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(data, text, clusters, labels, n_terms):\n",
    "    data = np.array(data)\n",
    "    clusters = np.array(clusters)\n",
    "    text_feature = text.toarray()\n",
    "    labels = np.array(labels)\n",
    "    group = pd.DataFrame(text.todense()).groupby(clusters)\n",
    "    #print(\"Data point in each group:\\n\", group.size())\n",
    "    df = group.mean() # [(clusters) rows x (feature) columns]\n",
    "\n",
    "    for i,r in df.iterrows():\n",
    "        sub_text = data[clusters == i]\n",
    "        sub_text_list = sub_text\n",
    "        sub_text_feature = text_feature[clusters == i]\n",
    "        #print(\"sub_text: {} | sub_text_feature: {}\".format(sub_text.shape,sub_text_feature.shape))\n",
    "\n",
    "        dist = np.linalg.norm(sub_text_feature - np.array(r), axis=1)\n",
    "\n",
    "        #print(\"Euclidean distance:\", dist.shape, dist)\n",
    "        \n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(', '.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n",
    "        #print(','.join([str(r[t]) for t in np.argsort(r)[-n_terms:]]))\n",
    "        print('\\n'.join([re.sub(' +', ' ',_filter(sub_text_list[t])) + \" | \" + str(dist[t]) for t in np.argsort(dist)[-2:]]))\n",
    "            \n",
    "get_top_keywords(reduced_data, sample_text, sample_text_labels, tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=tokens, tags=[str(i)]) for i, tokens in enumerate(token_data)]\n",
    "\n",
    "max_epochs = 100\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=max_features,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                workers=n_cpu,\n",
    "                dm=1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "bar = tqdm(range(max_epochs))\n",
    "for epoch in bar:\n",
    "    bar.set_description('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model= Doc2Vec.load(\"d2v.model\")\n",
    "model= Doc2Vec.load(\"enwiki_dbow/doc2vec.bin\")\n",
    "'''\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"I love chatbots\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)\n",
    "\n",
    "\n",
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar('1')\n",
    "print(similar_doc)\n",
    "\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "print(model.docvecs['1'])\n",
    "'''\n",
    "\n",
    "n_reduced_data = len(reduced_data)\n",
    "docvec = []\n",
    "for i in tqdm(range(n_reduced_data)):\n",
    "    #docvec.append(model.docvecs[str(i)])\n",
    "    docvec.append(model.infer_vector(token_data[i]))\n",
    "docvec = np.array(docvec)\n",
    "print(\"Number of data: {}\".format(n_reduced_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_optimal_clusters(docvec, max_k, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cluster_and_plot(docvec, \"kmeans\", 10, None, 50, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cluster_and_plot(docvec, \"dbscan\", None, 2.1, 50, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "* [Clustering documents with TFIDF and KMeans](https://www.kaggle.com/jbencina/clustering-documents-with-tfidf-and-kmeans)\n",
    "* [Analyzing tf-idf results in scikit-learn](https://buhrmann.github.io/tfidf-analysis.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Topic Model\n",
    "如果不移除 stopword 的話效果很差，主題的字都會是 of, for, it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(token_data)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in token_data]\n",
    "\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=n_cpu)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=n_cpu)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Sentence Encoding Clustering\n",
    "\n",
    "1. 要先把 bert server 開起來\n",
    "    `gpu0 bert-serving-start -model_dir uncased_L-12_H-768_A-12 -num_worker 4  -port 1355 -max_seq_len 40 -device_map 0`\n",
    "2. 直到出現 `all set, ready to serve request!` 才可以 run client 的 command\n",
    "Note:\n",
    "1. 如果是 run 大的 model 會 load 不進去 GPU 中，跑出 OOM\n",
    "2. 如果系統記憶體不夠則不會出現 all set, ready to serve request!，CPU 版來說，最多只能 -num_worker 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient(port=1355)\n",
    "print(\"Start predicting\")\n",
    "bert_output = bc.encode(clean_data)\n",
    "bert_output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_data_path = \"bert_base.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_data = {\n",
    "    \"clean_data\": clean_data,\n",
    "    \"reduced_data\": reduced_data,\n",
    "    \"token_data\": token_data,\n",
    "    \"bert_data\": bert_output\n",
    "}\n",
    "with open(bert_data_path, \"wb\") as f:\n",
    "    pickle.dump(bert_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bert_data_path, \"rb\") as f:\n",
    "    bert_data = pickle.load(f)\n",
    "clean_data = bert_data[\"clean_data\"]\n",
    "reduced_data = bert_data[\"reduced_data\"]\n",
    "token_data = bert_data[\"token_data\"]\n",
    "bert_output = bert_data[\"bert_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_optimal_clusters(bert_output, max_k, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = cluster_and_plot(bert_output, \"kmeans\", 10, None, 50, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = cluster_and_plot(bert_output, \"dbscan\", None, 5.95, 50, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
