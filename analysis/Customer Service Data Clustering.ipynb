{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Service Data Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"..\",\"data\",\"./newdata_clean.xlsx\")\n",
    "n_cpu = 15\n",
    "batch_size = 10000\n",
    "max_k = 40\n",
    "max_features = 256  # only consider the top max_features ordered by term frequency across the corpus.\n",
    "loadpath = \"processed_data_lda_wo_html\"\n",
    "#loadpath = \"processed_data_not_rmsw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(loadpath, \"rb\") as f:\n",
    "    output = pickle.load(f)\n",
    "clean_data = output[\"clean_data\"]\n",
    "reduced_data = output[\"reduced_data\"]\n",
    "token_data = output[\"token_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(data, max_k, batch_size):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    bar = tqdm(iters)\n",
    "    for k in bar:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, batch_size=batch_size, random_state=20).fit(data).inertia_)\n",
    "        bar.set_description('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_plot(data, cluster_algo, n_clusters, eps, calculate_portion, draw_portion):\n",
    "    n_data = data.shape[0]\n",
    "    print(\"Input Data shape:\", data.shape)\n",
    "    \n",
    "    # random sample (n_sample) points\n",
    "    np.random.seed(5)\n",
    "    n_sample = n_data // calculate_portion\n",
    "    print(\"Only calculate {} data points\".format(n_sample))\n",
    "    sample_items = np.random.choice(range(n_data), size=n_sample, replace=False)\n",
    "    \n",
    "    \n",
    "    if cluster_algo == \"kmeans\":\n",
    "        labels = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, random_state=20).fit_predict(data)\n",
    "        npdata = data[sample_items,:]\n",
    "        label_subset = labels[sample_items]\n",
    "    elif cluster_algo == \"dbscan\":\n",
    "        npdata = data[sample_items]\n",
    "        labels = DBSCAN(eps=eps, n_jobs=n_cpu).fit_predict(npdata)\n",
    "        label_subset = labels\n",
    "        print(\"Number of noise data: {}\".format(len(label_subset[label_subset == -1])))\n",
    "    \n",
    "    # number of clusters\n",
    "    max_label = max(labels) \n",
    "    print(\"Number of clusters: {}\".format(max_label + 1))\n",
    "    pca = PCA(n_components=2, whiten=True).fit_transform(npdata)\n",
    "    print(\"PCA done\")\n",
    "    tsne_pca = TSNE(n_jobs=n_cpu, n_iter=5000).fit_transform(PCA(n_components=(max_features // 2)).fit_transform(npdata))\n",
    "    print(\"tsne_pca done\")\n",
    "    tsne = TSNE(n_jobs=n_cpu, n_iter=5000).fit_transform(npdata)\n",
    "    print(\"tsne30 done\")\n",
    "    tsne_per50 = TSNE(n_jobs=n_cpu, perplexity=50, n_iter=5000).fit_transform(npdata)\n",
    "    print(\"tsne50 done\")\n",
    "    \n",
    "    # draw only (n_draw) points\n",
    "    np.random.seed(5)\n",
    "    n_draw = n_sample // draw_portion\n",
    "    print(\"Only draw {} data points\".format(n_draw))\n",
    "    idx = np.random.choice(range(pca.shape[0]), size=n_draw, replace=False)\n",
    "    \n",
    "    # draw scatter\n",
    "    if cluster_algo == \"kmeans\":\n",
    "        start_idx = 0\n",
    "    else:\n",
    "        start_idx = -1\n",
    "    f, ax = plt.subplots(2, 2, figsize=(13, 10))\n",
    "    for i in range(start_idx,max_label + 1):\n",
    "        sub_idx = idx[label_subset[idx] == i]\n",
    "        #print(\"{} points in group {}\".format(len(sub_idx),i))\n",
    "        label_subset_color = np.array([cm.hsv(i/ ( max_label + 1)) for i in label_subset[sub_idx]])\n",
    "        ax[0,0].scatter(pca[sub_idx, 0], pca[sub_idx, 1])   # , c=label_subset_color\n",
    "        ax[0,0].set_title('PCA Cluster Plot')\n",
    "        \n",
    "        ax[1,0].set_title('t-SNE & PCA Cluster Plot')\n",
    "        ax[1,0].scatter(tsne_pca[sub_idx, 0], tsne_pca[sub_idx, 1], label=\"Group {} | {}\".format(i,len(sub_idx))) # , c=label_subset_color\n",
    "        \n",
    "        ax[0,1].set_title('t-SNE Cluster Plot')\n",
    "        ax[0,1].scatter(tsne[sub_idx, 0], tsne[sub_idx, 1]) # , c=label_subset_color\n",
    "        \n",
    "        ax[1,1].set_title('t-SNE Cluster Plot (Perplexity 50)')\n",
    "        ax[1,1].scatter(tsne_per50[sub_idx, 0], tsne_per50[sub_idx, 1]) # , c=label_subset_color\n",
    "    f.legend() # plot only one legend\n",
    "    \n",
    "    if cluster_algo == \"kmeans\":\n",
    "        return data, labels\n",
    "    else:\n",
    "        return npdata, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max_feature\",max_features)\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df = 0.001,\n",
    "    max_df = 0.95,\n",
    "    max_features = max_features,\n",
    "    stop_words = 'english'\n",
    ")\n",
    "tfidf.fit(clean_data)\n",
    "text = tfidf.transform(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_optimal_clusters(text, max_k, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text, sample_text_labels = cluster_and_plot(text, \"kmeans\", 10, None, 500, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_and_plot(text, \"dbscan\", None, 0.999, 50, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get keywords of each TF-IDF cluster\n",
    "作法是將每個 cluster 的前幾高 tfidf 字取出來，作為這個 cluster 的代表字。但是效果不好，且很多 cluster 都有相同常見的字(ex: download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(data, text, clusters, labels, n_terms):\n",
    "    data = np.array(data)\n",
    "    clusters = np.array(clusters)\n",
    "    text_feature = text.toarray()\n",
    "    labels = np.array(labels)\n",
    "    group = pd.DataFrame(text.todense()).groupby(clusters)\n",
    "    #print(\"Data point in each group:\\n\", group.size())\n",
    "    df = group.mean() # [(clusters) rows x (feature) columns]\n",
    "\n",
    "    for i,r in df.iterrows():\n",
    "        sub_text = data[clusters == i]\n",
    "        sub_text_list = sub_text\n",
    "        sub_text_feature = text_feature[clusters == i]\n",
    "        #print(\"sub_text: {} | sub_text_feature: {}\".format(sub_text.shape,sub_text_feature.shape))\n",
    "\n",
    "        dist = np.linalg.norm(sub_text_feature - np.array(r), axis=1)\n",
    "\n",
    "        #print(\"Euclidean distance:\", dist.shape, dist)\n",
    "        \n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(', '.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n",
    "        #print(','.join([str(r[t]) for t in np.argsort(r)[-n_terms:]]))\n",
    "        print('\\n'.join([re.sub(' +', ' ',_filter(sub_text_list[t])) + \" | \" + str(dist[t]) for t in np.argsort(dist)[-2:]]))\n",
    "            \n",
    "get_top_keywords(reduced_data, sample_text, sample_text_labels, tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=tokens, tags=[str(i)]) for i, tokens in enumerate(token_data)]\n",
    "\n",
    "max_epochs = 100\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=max_features,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                workers=n_cpu,\n",
    "                dm=1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "bar = tqdm(range(max_epochs))\n",
    "for epoch in bar:\n",
    "    bar.set_description('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model= Doc2Vec.load(\"d2v.model\")\n",
    "model= Doc2Vec.load(\"enwiki_dbow/doc2vec.bin\")\n",
    "'''\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"I love chatbots\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)\n",
    "\n",
    "\n",
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar('1')\n",
    "print(similar_doc)\n",
    "\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "print(model.docvecs['1'])\n",
    "'''\n",
    "\n",
    "n_reduced_data = len(reduced_data)\n",
    "docvec = []\n",
    "for i in tqdm(range(n_reduced_data)):\n",
    "    #docvec.append(model.docvecs[str(i)])\n",
    "    docvec.append(model.infer_vector(token_data[i]))\n",
    "docvec = np.array(docvec)\n",
    "print(\"Number of data: {}\".format(n_reduced_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_optimal_clusters(docvec, max_k, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cluster_and_plot(docvec, \"kmeans\", 10, None, 50, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cluster_and_plot(docvec, \"dbscan\", None, 2.1, 50, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "* [Clustering documents with TFIDF and KMeans](https://www.kaggle.com/jbencina/clustering-documents-with-tfidf-and-kmeans)\n",
    "* [Analyzing tf-idf results in scikit-learn](https://buhrmann.github.io/tfidf-analysis.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Sentence Encoding Clustering\n",
    "\n",
    "1. 要先把 bert server 開起來\n",
    "    `gpu0 bert-serving-start -model_dir uncased_L-12_H-768_A-12 -num_worker 4  -port 1355 -max_seq_len 40 -device_map 0`\n",
    "2. 直到出現 `all set, ready to serve request!` 才可以 run client 的 command\n",
    "Note:\n",
    "1. 如果是 run 大的 model 會 load 不進去 GPU 中，跑出 OOM\n",
    "2. 如果系統記憶體不夠則不會出現 all set, ready to serve request!，CPU 版來說，最多只能 -num_worker 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient(port=1355)\n",
    "print(\"Start predicting\")\n",
    "bert_output = bc.encode(clean_data)\n",
    "bert_output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_data_path = \"bert_base.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_data = {\n",
    "    \"clean_data\": clean_data,\n",
    "    \"reduced_data\": reduced_data,\n",
    "    \"token_data\": token_data,\n",
    "    \"bert_data\": bert_output\n",
    "}\n",
    "with open(bert_data_path, \"wb\") as f:\n",
    "    pickle.dump(bert_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bert_data_path, \"rb\") as f:\n",
    "    bert_data = pickle.load(f)\n",
    "clean_data = bert_data[\"clean_data\"]\n",
    "reduced_data = bert_data[\"reduced_data\"]\n",
    "token_data = bert_data[\"token_data\"]\n",
    "bert_output = bert_data[\"bert_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_optimal_clusters(bert_output, max_k, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = cluster_and_plot(bert_output, \"kmeans\", 10, None, 50, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = cluster_and_plot(bert_output, \"dbscan\", None, 5.95, 50, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
