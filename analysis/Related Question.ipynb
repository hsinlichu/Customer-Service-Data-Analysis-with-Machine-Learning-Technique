{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related Question\n",
    "這個主題是因為想要透過類似主題的方式，讓客人在還沒有將問題提交前就可以得到類似問題的解答，以期可以解決簡單的客人問題，並且減少客訴的量。\n",
    "這邊我使用的方式是使用 Bert 來做骨幹架構，來表示出 sentence。\n",
    "其中 Bert 我是使用 [bert-as-service](https://github.com/hanxiao/bert-as-service) 套件，搭配 [BERT-Base, Uncased(12-layer, 768-hidden, 12-heads, 110M parameters)](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip) pretrained model 輸出 sentence representation。\n",
    "\n",
    "我會選擇上面那個 model 單純只是因為 GPU 記憶體只塞得進這個 model 的關係，如果用更大的 model 可想而知效果應該會更好。\n",
    "\n",
    "使用流程：\n",
    "1. Install require package\n",
    "\n",
    "    ```\n",
    "    pip install bert-serving-server  # server\n",
    "    pip install bert-serving-client  # client, independent of `bert-serving-server`\n",
    "    ```\n",
    "2. Download pretrain model\n",
    "3. Start the BERT service\n",
    "    在同一台電腦的 shell 輸入底下的 command，並且直到 shell 輸出 all set, ready to serve request!\n",
    "    另外 num_worker 會牽扯到記憶體用量，如果一直沒有輸出 all workers ready，有可能就是因為記憶體不夠的關係。\n",
    "    \n",
    "    `bert-serving-start -model_dir uncased_L-12_H-768_A-12 -num_worker 3  -port 1355 -max_seq_len 150 -device_map 3 -show_tokens_to_client`\n",
    "4. Run this jupyter notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "loadpath = \"processed_data_bert_expand\"\n",
    "bert_data_path = \"bert_expand.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀取 Data Preprocessing.ipynb 已經預處理完的資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(loadpath, \"rb\") as f:\n",
    "    output = pickle.load(f)\n",
    "clean_data = output[\"clean_data\"]\n",
    "reduced_data = output[\"reduced_data\"]\n",
    "token_data = output[\"token_data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "開始取得 dataset 中每個句子的 sentence representation。處理時間會因為 `n_worker` 的數量以及 gpu 的運算能力而有差別，我自己是在 GeForce GTX 1080 Ti 上面 n_worker=4，共跑了約 3 個小時。\n",
    "\n",
    "如果連線成功在你 run `bert-serving-start` 的那個 shell 應該會有一堆 log 出現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient(port=1355)\n",
    "print(\"Start predicting\")\n",
    "bert_output = bc.encode(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將好不容易跑出來的結果儲存起來，之後使用就不需要重新跑一遍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_data = {\n",
    "    \"clean_data\": clean_data,\n",
    "    \"reduced_data\": reduced_data,\n",
    "    \"token_data\": token_data,\n",
    "    \"bert_data\": bert_output\n",
    "}\n",
    "with open(bert_data_path, \"wb\") as f:\n",
    "    pickle.dump(bert_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dataset with bert sentence representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bert_data_path, \"rb\") as f:\n",
    "    bert_data = pickle.load(f)\n",
    "clean_data = bert_data[\"clean_data\"]\n",
    "reduced_data = bert_data[\"reduced_data\"]\n",
    "token_data = bert_data[\"token_data\"]\n",
    "bert_output = bert_data[\"bert_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'numpy.ndarray'> (100910, 768)\n",
      "torch.Size([100910, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Type: \", type(bert_output), bert_output.shape)\n",
    "bert_tensor = torch.from_numpy(bert_output)\n",
    "print(bert_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "這邊模擬 testing 的情境，當有一個新的 query sentence，要先把句子預處理完後再丟進 `predict()` function。這邊我偷懶就直接拿之前已經預處理好的句子丟進去。\n",
    "\n",
    "要注意要執行 `predict()` function 前還是要在 shell 用 `bert-serving-start` 把 model run 起來。\n",
    "\n",
    "`predict()` function 會先取得 query sentence 的 sentence representation，接著再與先前 dataset 取得的 Bert sentence representation 去算 cosine similarity，數值越高就與現在這個 query sentence 越相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient(port=1355)\n",
    "bert_norm = bert_tensor / torch.norm(bert_tensor, dim=1).view(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "def predict(test_sentence, num_related, ignore_first=False):\n",
    "    test_sentence = test_sentence.lower()\n",
    "    print(\"Query: {}\".format(test_sentence))\n",
    "    test_array, token = bc.encode([test_sentence], show_tokens=True)\n",
    "    #print(token)\n",
    "    test_tensor = torch.tensor(test_array[0])\n",
    "    #print(\"bert_tensor:\", bert_tensor.size()) # torch.Size([100868, 768])\n",
    "    \n",
    "    test_norm = test_tensor / torch.norm(test_tensor)\n",
    "    similarity = torch.matmul(bert_norm, test_norm.view(-1,1))\n",
    "    \n",
    "    rank = torch.argsort(similarity, dim=0, descending=True)\n",
    "    start = 1 if ignore_first else 0\n",
    "\n",
    "    for i in range(start, num_related + start):\n",
    "        print(\"\\n\" + \"=\" * 10 + \"Similarity: {}\".format(similarity[rank[i]][0][0]) + \"=\" * 10)\n",
    "        print(re.sub(r'<[^<]*?/?>', '', reduced_data[rank[i]])) # remove output sentence html \n",
    "        #print()\n",
    "        #print(clean_data[rank[i]])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i, index in enumerate(random.sample(range(len(clean_data)), 5)):\n",
    "    #print(\"Query: {}\".format(reduced_data[index]))\n",
    "    predict(clean_data[index], 3, ignore_first=True)\n",
    "    print(\"\\n\" + \"*\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "這邊為了增加 demo 的互動性，我用一個 `while` 迴圈讓使用者可以一直輸入 query 的句子，直到使用者輸入 `EOF`。\n",
    "這邊事實上其實應該要和 training data 一樣將使用者輸入的句子經過相同的前處理，也就是 `Data Preprocessing.ipynb` 檔案中 `preprocess()` functoion，不過這邊為了簡潔就省略了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    input_sentence = input(\"Please type your question here:\") # I cannot activate my PowerDVD.\n",
    "    if input_sentence == \"EOF\":\n",
    "        break\n",
    "    predict(input_sentence, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "從最後輸出的結果來看其實還不錯，可是再經過一些調查後發現 bert 並不適合這樣直接當作 sentence encoder，目前想到的解決方法如下。\n",
    "\n",
    "1. 先 find tune 在一些 task 上，像是最一開始做得 supervised classification 後再拿 `[CLS]` 的 output 作為 sentence represention。\n",
    "2. 使用 [Universal Sentence Encoder](https://arxiv.org/pdf/1803.11175.pdf)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
