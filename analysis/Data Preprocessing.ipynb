{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "將原始的資料做前處理，最後輸出成 pickle 檔供後續 model、分析使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path: raw data 存放的位置\n",
    "\n",
    "savepath: 處理好的資料儲存檔名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"..\",\"data\",\"./newdata_clean.xlsx\")\n",
    "n_cpu = 4\n",
    "batch_size = 10000\n",
    "\n",
    "savepath = \"processed_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "使用 catName 當作資料的 label，可以用來做 supervised 的訓練。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(path)\n",
    "\n",
    "df = df.dropna() # drop nan entry\n",
    "# df[pd.isnull(df).any(axis=1)]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['catName'].unique())\n",
    "num_classes = len(le.classes_)\n",
    "class_list = list(le.classes_)\n",
    "\n",
    "#print(class_list)\n",
    "\n",
    "print(\"number of classes:\",num_classes)\n",
    "df.loc[:,'catName'] = le.transform(df.loc[:,'catName'])\n",
    "data = df.question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess function\n",
    "下方的 function 為前處理不同的項目，可以視情況選用，像是 bert 我就沒有使用下方 4, 6, 7, 以及部分 `_filter()` 內容。\n",
    "1. `_expandContractions()`:\n",
    "    會根據 cList 的內容將出現在句子中的縮寫展開。\n",
    "2. `_removeRedundant`:\n",
    "    因為原始的資料來自數個產品，往往會夾帶許多不一樣但是制式的內容，因此這邊利用 regular expression 找到會將之濾掉。\n",
    "3. `_filter()`:\n",
    "    這個 function 會把像是網址、上傳檔案等冗於資訊濾掉，同時因為對於像是 tfidf 是對不同字做處理，所以把像是 \"power dvd\" 合成一個專有名詞 \"powerdvd\"。\n",
    "4. `_correct_word()`:\n",
    "    這個 function 會嘗試把打錯的字校正，然而速度非常慢！\n",
    "6. `_lemmatization()`:\n",
    "    這個 function 會把動詞的三態還原、複數名詞轉成單數等\n",
    "    它的原理是先用 `_get_wordnet_pos()` 將字做詞性的分類，之後再根據詞性去掉字尾。\n",
    "7. `_remove_stopword()`:\n",
    "    這個 function 會以 nltk 預設的 english 為基準去掉句子中的 stopword。可是因為並不是非常足夠，所以有加上我自己觀察資料所挑出的 stopword，以及人名的 list。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "def _expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _removeRedundant(x):\n",
    "    log = []\n",
    "    redundant_pos = re.search(r\"dear\\s+valued\\s+customer,\", x)\n",
    "    if redundant_pos != None:\n",
    "        log.append(\"cut dear valued customer,\")\n",
    "        x = x[:redundant_pos.start()]\n",
    "    redundant_pos = re.search(r\"order\\s+confirmation\\s+order\\s+number:\", x)\n",
    "    if redundant_pos != None:\n",
    "        log.append(\"cut order confirmation order number:\")\n",
    "        x = x[:redundant_pos.start()]\n",
    "    redundant_pos = re.search(r\"<li>comment:</li>\", x)\n",
    "    if redundant_pos != None:\n",
    "        log.append(\"cut <li>comment:</li>\")\n",
    "        x = x[redundant_pos.end():]\n",
    "    redundant_pos = re.search(r\"time\\s+of\\s+this\\s+report:\", x)\n",
    "    if redundant_pos != None:\n",
    "        log.append(\"cut time of this report:\")\n",
    "        x = x[:redundant_pos.start()]\n",
    "    redundant_pos = re.search(r\"this\\s+correspondence\\s+is\\s+from\", x)\n",
    "    if redundant_pos != None:\n",
    "        log.append(\"cut this correspondence is from\")\n",
    "        x = x[:redundant_pos.start()]\n",
    "    redundant_pos = re.search(r\"forwarded\\s+message\", x)\n",
    "    if redundant_pos != None:\n",
    "        log.append(\"cut forwarded message\")\n",
    "        x = x[:redundant_pos.start()]\n",
    "    redundant_pos = re.search(r\"(best)?(kind)?\\s+regards\", x)\n",
    "    if redundant_pos != None:\n",
    "        log.append(\"cut best|kind regards\")\n",
    "        x = x[:redundant_pos.start()]\n",
    "    redundant_pos = re.search(r\"media\\s+source\\s+error\\s+report\", x)\n",
    "    if redundant_pos != None:\n",
    "        log.append(\"cut Media Source Error Report\")\n",
    "        x = x[:redundant_pos.start()]\n",
    "    redundant_pos = re.search(r\"<br><br>Attach\\s+File\\s+:\", x)\n",
    "    if redundant_pos != None:\n",
    "        log.append(\"cut Attach File\")\n",
    "        x = x[:redundant_pos.start()]\n",
    "    #print(\"\\n\".join(log))\n",
    "    return x\n",
    "\n",
    "\n",
    "def _filter(x):      \n",
    "    x = re.sub(r'<[^<]*?/?>', ' ', x)                              # remove all html tag\n",
    "    x = re.sub(r\"\\w{5}(-\\w{5}){5}\", \"[KEY]\", x)                    # replace key tokex, ex: sd2kk-33j7v-na4m2-m8n5s-sjaxq-bramm\n",
    "    x = re.sub(r'https?:\\/\\/[^ ]*', ' ', x)                        # remove all url\n",
    "    x = re.sub(r'\\S*@\\S*\\s?', ' ', x)                              # remove all email address\n",
    "    x = re.sub(r'\\S*\\.\\S*\\s?', ' ', x, flags=re.IGNORECASE)        # remove all filename\n",
    "    #x = re.sub(r'[^a-z A-Z]', ' ', x)                             # remove all non-english alphabat\n",
    "    '''\n",
    "    x = re.sub(r\"power\\s+dvd\", \"powerdvd\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"power\\s+director\", \"powerdirector\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"audio\\s+director\", \"audiodirector\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"color\\s+director \", \"colordirector \", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"action\\s+director\", \"actiondirector\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"makeup\\s+director \", \"makeupdirector \", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"director\\s+suite\", \"directorsuite\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"audio\\s+director\", \"audiodirector\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"photo\\s+director\", \"photodirector \", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"blue?[-\\s]*rays?\", \"bluray\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"power\\s*(2|(to))\\s*go\", \"power2go\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"cyber\\s+link\", \"cyberlink\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"pdr\", \"powerdirector\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"pdvd\", \"powerdvd\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"pls\", \"please\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"add[-\\s]*ons?\", \"addon\", x, flags=re.IGNORECASE)\n",
    "    x = re.sub(r\"media[-\\s]*suite\", \"mediasuite\", x, flags=re.IGNORECASE)\n",
    "    '''\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _correct_word(text1):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    text2 = pattern.sub(r\"\\1\\1\", text1) # reduce lengthening\n",
    "    #if text1 != text2:\n",
    "    #    print(text1, text2)\n",
    "    text3 = spell(text2).lower() # spell correction\n",
    "    #if text2 != text3:\n",
    "    #    print(text2, text3)\n",
    "    return text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/student/05/b05505004/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import wordnet\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def _get_wordnet_pos(tag):\n",
    "    if tag =='J':\n",
    "        return wordnet.ADJ\n",
    "    elif tag =='V':\n",
    "        return wordnet.VERB\n",
    "    elif tag =='N':\n",
    "        return wordnet.NOUN\n",
    "    elif tag =='R':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def _lemmatization(tokens):\n",
    "    tagged_sent = nltk.pos_tag(tokens) # [('The', 'DT'), ('striped', 'JJ'), ('for', 'IN'), ('best', 'JJS')]\n",
    "    ret = []\n",
    "    for tag in tagged_sent:\n",
    "        wordnet_pos = _get_wordnet_pos(tag[1][0]) or wordnet.NOUN\n",
    "        ret.append(wnl.lemmatize(tag[0], pos=wordnet_pos))\n",
    "        #print(tag[0],tag[1][0],wordnet_pos,ret[-1])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'nbsp', 'would', 'cant', 'hey', 'quot', 'dont', 'cyberlink', 'guy', 'wont', 'didnt', 'doesnt']\n",
      "Stopwords length: 190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student/05/b05505004/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "namelist = [\"paul\", \"jim\", \"larry\", \"ken\", \"wright\", \"peter\", \"donna\", \"ian\", \"rick\", \"richard\", \"william\", \"john\", \"chris\", \"tony\", \"joseph\"]\n",
    "stw = nltk.corpus.stopwords.words('english') + [\"nbsp\", \"would\", \"cant\", \"hey\", \"quot\", \"dont\", \"cyberlink\", \"guy\", \"wont\", \"didnt\", \"doesnt\"]\n",
    "print(stw)\n",
    "print(\"Stopwords length: {}\".format(len(stw)))\n",
    "def _remove_stopword(tokens):\n",
    "    ret = []\n",
    "    for word in tokens:\n",
    "        if word in namelist:\n",
    "            print(\"skip name {}\".format(word))\n",
    "            continue\n",
    "        if word not in stw and len(word) > 2:\n",
    "            ret.append(word)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下方的區塊正式會將資料做前處理，因為資料過多，所以有使用 multi-process 加速。\n",
    "1. 因為資料裡混有像是法文、韓文等非英文語言，所以使用 `langid` 套件把非英文語言濾掉\n",
    "2. 也有一個 `if` 可以濾掉過短句子\n",
    "\n",
    "\n",
    "* `clean_data`: list 裡面的內容是原始 data 的句子。換句話說沒有經過任何預處理，僅濾掉過短、非英文的句子。\n",
    "* `reduced_data`: list 裡的內容是 clean_data 的句子經過預處理。也就是根據選擇的 function 去掉 stopword、還原三態...。\n",
    "* `token_data`: list 裡的內容是 reduced_data 的句子以空白 token 過的 list。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin Data length: 106478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26619/26619 [05:47<00:00, 76.63it/s] \n",
      "100%|██████████| 26619/26619 [05:58<00:00, 88.52it/s] \n",
      "100%|██████████| 26621/26621 [06:00<00:00, 73.78it/s] \n",
      "100%|█████████▉| 26571/26619 [06:01<00:00, 89.65it/s]\n",
      "100%|██████████| 26619/26619 [06:02<00:00, 73.45it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = _removeRedundant(sentence)\n",
    "    sentence = _expandContractions(sentence)\n",
    "    sentence = _filter(sentence)\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    #tokens = [self._correct_word(word) for word in tokens] # spell correction\n",
    "    #tokens = _lemmatization(tokens) # lemmatization\n",
    "    #tokens = _remove_stopword(tokens) # remove stopwords\n",
    "    #s = \" \".join(tokens)\n",
    "    s = sentence ### for BERT ONLY\n",
    "    return s, tokens\n",
    "\n",
    "\n",
    "import langid\n",
    "def process_batch(batch):   \n",
    "    clean_batch = []\n",
    "    reduced_batch = []\n",
    "    token_batch = []\n",
    "    for ori_s in tqdm(batch):\n",
    "        s = ori_s.lower()\n",
    "        ret = langid.classify(s)\n",
    "        if ret[0] != \"en\" and ret[1] < -100: # remove language other than english\n",
    "            continue\n",
    "\n",
    "        processed, tokens = preprocess(s)\n",
    "        if len(tokens) <= 5: # remove too short sentence\n",
    "            continue\n",
    "        \n",
    "        reduced_batch.append(ori_s)\n",
    "        clean_batch.append(processed)\n",
    "        token_batch.append(tokens)\n",
    "    return clean_batch, reduced_batch, token_batch\n",
    "\n",
    "clean_data = []\n",
    "reduced_data = []\n",
    "token_data = []\n",
    "\n",
    "n_workers = n_cpu\n",
    "from multiprocessing import Pool\n",
    "ret = [None] * n_workers\n",
    "n_data = len(data)\n",
    "print(\"Origin Data length:\",n_data)\n",
    "with Pool(processes=n_workers) as pool:\n",
    "    for i in range(n_workers):\n",
    "        batch_start = (n_data // n_workers) * i \n",
    "        if i == n_workers - 1:\n",
    "            batch_end = n_data\n",
    "        else:\n",
    "            batch_end = (n_data // n_workers) * (i + 1)\n",
    "        batch = data[batch_start:batch_end]\n",
    "        ret[i] = pool.apply_async(process_batch, [batch])\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "for result in ret:\n",
    "    clean_batch, reduced_batch, token_batch = result.get()\n",
    "    clean_data += clean_batch\n",
    "    reduced_data += reduced_batch\n",
    "    token_data += token_batch\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    \"clean_data\": clean_data,\n",
    "    \"reduced_data\": reduced_data,\n",
    "    \"token_data\": token_data\n",
    "}\n",
    "with open(savepath, \"wb\") as f:\n",
    "    pickle.dump(output, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
